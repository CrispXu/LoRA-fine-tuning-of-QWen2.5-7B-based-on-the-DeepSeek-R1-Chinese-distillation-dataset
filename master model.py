import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ---------------------------
# è®¾å¤‡é…ç½®
# ---------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"

# ---------------------------
# æ¨¡å‹ä¸åˆ†è¯å™¨åŠ è½½
# ---------------------------
model_path = "Qwen2.5-7B"  # ğŸ¤— Hubå®˜æ–¹è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    pad_token="<|endoftext|>"  # æ˜¾å¼è®¾ç½®å¡«å……token
)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    trust_remote_code=True
).eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# ---------------------------
# æ ¸å¿ƒå¯¹è¯å‡½æ•°
# ---------------------------
def qwen_base_chat(prompt, max_new_tokens=512, temperature=0.7):
    """
    ä¸åŸºç¡€æ¨¡å‹å¯¹è¯
    :param prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜/æŒ‡ä»¤
    :param max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤512ï¼‰
    :param temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼ˆ0.0-1.0ï¼‰
    :return: æ¨¡å‹ç”Ÿæˆçš„å›ç­”
    """
    # æ„å»ºå¯¹è¯æ¨¡æ¿
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"},
        {"role": "user", "content": prompt}
    ]
    
    # åº”ç”¨å®˜æ–¹å¯¹è¯æ¨¡æ¿
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(text, return_tensors="pt").to(device)
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id
        )
    
    # è§£ç è¾“å‡º
    response = outputs[0][inputs.input_ids.shape[-1]:]
    return tokenizer.decode(response, skip_special_tokens=True)

# ---------------------------
# ç¤ºä¾‹è°ƒç”¨
# ---------------------------
if __name__ == "__main__":
    test_questions = [
        "è¯·è§£é‡Šç›¸å¯¹è®ºçš„åŸºæœ¬åŸç†",
        "ç”¨Pythonå®ç°å¿«é€Ÿæ’åºç®—æ³•",
        "å¦‚ä½•é¢„é˜²æ„Ÿå†’ï¼Ÿ"
    ]
    
    for question in test_questions:
        print(f"ç”¨æˆ·é—®é¢˜ï¼š{question}")
        print(f"æ¨¡å‹å›ç­”ï¼š{qwen_base_chat(question)}\n{'-'*50}")